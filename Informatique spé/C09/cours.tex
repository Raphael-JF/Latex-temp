\input{../../stock/en-tete_v4.tex}
\begin{document}
\begin{adjustwidth}{-3cm}{-3cm}
\input{../../stock/commands.tex}
\newcounter{chapitre}
\setcounter{chapitre}{9}
\title{Chapitre 9 : Grammaires non contextuelles}
\maketitle

\section{Grammaire non contextuelle (ou hors contexte)}

\subsection{Vocabulaire}

\begin{definition}{}{grammaire au sens général, grammaire de type 0}
    Une \notion{grammaire} est défini par un quadruplet $(\Sigma,\,V,\,P,\,S)$ où~:
    \begin{itemize}
        \item $\Sigma$ est un alphabet fini de \notion{symboles terminaux}, dit aussi \notion{alphabet terminal}
        \item $V$ est un alphabet fini de \notion{symboles non terminaux} (ou \notion{variables}), dit aussi \notion{alphabet non terminal}
        \item $P \subset (\Sigma \cup V)^* \times (\Sigma \cup V)^* $ est un ensemble de \notion{règles de production}.\\
        Une règle de production $(w_1,w_2) \in P$, notée $w_1 \to w_2$ est un couple de mots écrits avec des symboles terminaux et non terminaux.
        \item $S \in V$ est un symbole non terminal avec un statut particulier de \notion{symbole initial} (ou \notion{axiome}, \notion{variable initiale})
    \end{itemize}
    Une grammaire sans propriété particulière est dite \notion{type 0}.
\end{definition}

\begin{remarque}{}{grammaires}
    On note usuellement par des majuscules les symboles non terminaux, et en minuscule les terminaux.
\end{remarque}

\newcommand{\rp}[2]{\code{#1}\to\code{#2}}


\begin{exemple}{}{de grammaire de type 0}
    Pour $\Sigma = \{\code{a}\}$, $S = \code{S}$, $V = \{S,\code{D},\code{F},\code{X},\code{Y},\code{Z}\}$
    et $P = \{\rp{S}{DXaD},\,\rp{Xa}{aaX},\,\rp{XF}{YF},\,\rp{aY}{Ya},\,\rp{DY}{DX},\,\rp{XZ}{Z},\,\rp{aZ}{Za},\,\code{DZ}\to \epsilon\}$, $G = (\Sigma, V, P, S)$ est une grammaire de type 0.
\end{exemple}

\begin{definition}{}{dérivabilité immédiate}
    Soit $G = (\Sigma, V, P, S)$ une grammaire. Soit $\alpha$ et $\beta$ deux mots de $(\Sigma \cup V)^*$. On dit que \notion{$\alpha$ se dérive immédiatement en $\beta$} lorsqu'il existe $(\alpha_1,\beta_1)\in P$ tel que~:
    $$\exists (u, v) \in \Big((\Sigma \cup V)^*\Big)^2,\, \begin{cases*}
        \alpha = u\alpha_1v\\
        \beta = u \beta_1 v
    \end{cases*}$$
    Le cas échéant, on note $\alpha \Rightarrow  \beta$. On parle de \notion{dérivation immédiate}. Moralement, la règle de production $(\alpha_1,\beta_1)$ remplace le facteur $\alpha_1$ par le facteur $\beta_1$.
\end{definition}

\newcommand{\crt}{\Rightarrow ^*}


\begin{definition}{}{clôture reflexive et transitive}
    On note $\crt$ la \notion{clôture reflexive et transitive} de la relation $\Rightarrow $ de dérivabilité immédiate.\\\\
    $\crt$ est définie comme la plus petite relation au sens de l'inclusion tel que~:
    \begin{itemize}
        \item $\forall \alpha \in (\Sigma \cup V)^*,\, \alpha \crt \alpha$
        \item $\forall (\alpha,\beta) \in \Big((\Sigma \cup V)^*\Big)^2, (\alpha \Rightarrow  \beta) \implies (\alpha \crt \beta)$
        \item $\forall (\alpha,\beta, \gamma) \in \Big((\Sigma \cup V)^*\Big)^3,\, (\alpha \crt \beta \text{ et } \beta \crt \gamma) \implies (\alpha \crt \gamma)$
    \end{itemize}
    Autrement dit, $\alpha \crt \beta$ lorsqu'il existe $(\alpha=\alpha_0,\, \dots,\, \alpha_k = \beta)$ une suite de mots dans $(\Sigma \cup V)^*$ telle que~: 
    $$\forall i \in \intint{0}{k-1}, \alpha_i \Rightarrow  \alpha_{i+1}$$
\end{definition}

\begin{definition}{}{clôture reflexive et transitive de $\Rightarrow$}
    Soit $G = (\Sigma, V, P, S)$ une grammaire. Soit $\alpha$ et $\beta$ deux mots de $(\Sigma \cup V)^*$. On note \notion{$\crt$ la clôture reflexive et transitive de la relation $\Rightarrow $}. Ainsi $\alpha \crt \beta$ lorsqu'il existe $(\alpha=\alpha_0,\, \dots,\, \alpha_k = \beta)$ une suite de mots dans $(\Sigma \cup V)^*$ telle que~: 
    $$\forall i \in \intint{0}{k-1}, \alpha_i \Rightarrow  \alpha_{i+1}$$
\end{definition}

\begin{exemple}{}{de dérivation}
    Dans la grammaire précédemment introduite~:\\
    Pour $\Sigma = \{a\}$ et $V = \{S,D,F,X,Y,Z\}$
    et $P = \{\rp{S}{DXaD},\rp{Xa}{aaX},\rp{XF}{YF},\rp{aY}{Ya},\rp{DY}{DX},\rp{XZ}{Z},\rp{aZ}{Za},\code{DZ}\to \epsilon\}$, $G = (\Sigma, V, P, S)$ est une grammaire de type 0.
    \begin{align*}
        S &\Rightarrow \code{DXaF} \\
        &\Rightarrow \code{DaaXF}\\
        &\Rightarrow \code{DaaYF}\\
        &\Rightarrow \code{DaYaF}\\
        &\Rightarrow \code{DYaaF}\\
        &\Rightarrow \code{DXaaF}\\
        &\Rightarrow \code{DaaXaF}\\
        &\Rightarrow \code{DaaaaXF}\\
        &\Rightarrow \code{DaaaaZ}\\
        &\Rightarrow \code{DaaaZa}\\
        &\Rightarrow \code{DaaZaa}\\
        &\Rightarrow \code{DaZaaa}\\
        &\Rightarrow \code{DZaaaa}\\
        &\Rightarrow \code{aaaa}
    \end{align*}
    D'où $S \crt \code{aaaa}$
\end{exemple}

\begin{definition}{}{langage engendré, langage élargi engendré par une grammaire depuis un mot}
    Soit $G = (\Sigma, V, P , S)$ une grammaire et $\alpha \in (\Sigma\cup V)^*$.
    \begin{itemize}
        \item le \notion{langage engendré par $G$ depuis $\alpha$} est l'ensemble des mots de $\Sigma^*$ que l'on peut obtenir par dérivation de $\alpha$ en utilisant les règles de production de $G$~:
        $$\mc{L}_G(\alpha) = \{\beta \in \Sigma^*,\, \alpha \crt \beta\}$$
        \item le \notion{langage élargi engendré par $G$ depuis $\alpha$} est l'ensemble des mots de $(\Sigma \cup V)^*$ que l'on peut obtenir par dérivation de $\alpha$ en utilisant les règles de production de $G$~:
        $$\widehat{\mc{L}_G(\alpha)} = \{\beta \in (\Sigma \cup V)^*,\, \alpha \crt \beta\}$$
    \end{itemize}
\end{definition}

\begin{definition}{}{langage engendré par une grammaire}
    Soit $G = (\Sigma, V, P , S)$ une grammaire et $\alpha \in (\Sigma\cup V)^*$. Le \notion{langage engendré par $G$} désigne $\mc{L}_G(S)$ le langage engendré par $G$ depuis son symbole initial $S$.
\end{definition}

\begin{exemple}{}{}
    Pour la grammaire de l'exemple, on pourrait montrer que $\mc{L}_G(S) = \{a^{2^n},\, n \in \N^*\}$
    On montre que ce langage n'est pas régulier (absurde + lemme de l'étoile tmtc)
\end{exemple}

\begin{definition}{}{langage de type 0}
    On dit qu'un \notion{langage est de type 0} s'il peut être engendré par une grammaire de type 0.
\end{definition}

\begin{theoreme}{}{de Chomsky (HP)}
    Les langages de type 0 sont exactement les langages récursivement énumérables, c'est-à-dire les langages reconnaissables par une machine de Turing.
\end{theoreme}

\begin{definition}{}{grammaire contextuelle (HP)}
    Une grammaire $G=(\Sigma, V,P,S)$ de type 0 est appelée \notion{grammaire contextuelle (ou de type 1 ou monotone)} lorsque~:
    $$\forall (\alpha,\beta) \in P \setminus \{(S,\epsilon)\},\, \abs{\alpha} \leq \abs{\beta}$$
    Moralement, tous les facteurs "produits" sont plus long que les facteurs remplacés.
\end{definition}

\begin{exemple}{}{de grammaire qui n'est pas "contextuelle"}
    la grammaire exemple définie par $\Sigma = \{a\}$ et $V = \{S,D,F,X,Y,Z\}$ et $P = \{\rp{S}{DXaD},\rp{Xa}{aaX},\rp{XF}{YF},\rp{aY}{Ya},\rp{DY}{DX},\rp{XZ}{Z},\rp{aZ}{Za},\code{DZ}\to \epsilon\}$ puis $G = (\Sigma, V, P, S)$ n'est pas contextuelle ~:
    $$(\code{DZ}\rightarrow \epsilon) \in P$$
    mais~:
    $$\abs{DZ} = 2 > \abs{\epsilon} = 0$$
\end{exemple}

\begin{remarque}{}{indépendance des caractères "contextuel" et "non contextuel"}
    Nous le reverrons, mais une grammaire est "contextuelle" indépendamment de son caractère "non contextuel".
\end{remarque}

\begin{remarque}{}{}
    Sans l'autorisation d'avoir $(S,\epsilon) \in P$,\, les langages engendrés par des grammaires contextuelles ne peuvent pas contenir $\epsilon$.\\
    En effet, on a initialement $S$ de longueur 1 et les règles de productions ne peuvent qu'augmenter la longueur du mot. 0 serait donc une longueur inexistente.
\end{remarque}

\begin{definition}{}{grammaire non contextuelle}
    Une grammaire $G = (\Sigma, V, P, S)$ est dite \notion{non contextuelle} (ou \notion{hors contexte} ou encore \notion{algébrique} ou \notion{de type 2}) lorsque~:
    $$P \subset V \times (\Sigma \cup V)^*$$
    Moralement, les règles de production ne permettent de remplacer que des symboles non terminaux (et pas des mots) : des majuscules.
\end{definition}

\begin{exemple}{}{de règle de production non valide pour une grammaire "non contextuelle"}
    $\code{DZ}\rightarrow \epsilon$ n'est pas une règle possible pour une grammaire hors contexte.
\end{exemple}

\begin{exemple}{}{de grammaire "non contextuelle"}
    $G_1 = (\Sigma_1, V_1, P_1, S_1)$ définie par~:
    \begin{itemize}
        \item $\Sigma_1 = \{a,b\}$
        \item $V_1 = \{S_1\}$
        \item $P_1 = \{S_1 \rightarrow a S_1 b,\,S_1\rightarrow \epsilon\}$
    \end{itemize}
    est une grammaire hors contexte.
\end{exemple}

\begin{remarque}{}{}
    On dit "hors contexte" car les symboles non terminaux sont considérés seuls par les règles de production : on ne regarde pas les lettres autour.
\end{remarque}

\newcommand{\leng}[2]{\mc{L}_{#1}(#2)}


\begin{exemple}{}{langage engendré par une grammaire hors contexte}
    $G_1 = (\Sigma_1, V_1, P_1, S_1)$ définie par~:
    \begin{itemize}
        \item $\Sigma_1 = \{a,b\}$
        \item $V_1 = \{S_1\}$
        \item $P_1 = \{S_1 \rightarrow a S_1 b,\,S_1\rightarrow \epsilon\}$
    \end{itemize}
    Alors $\mc{L}_{G_1}(S_1) = \{a^nb^n,\, n \in \N\}$. On le démontre (Démo 1).
    Par ailleurs $\widehat{\mc{L}_{G_1}(S_1)} = \{a^n S_1 b^n,\, n \in \N\} \cup \{a^n b^n ,\, n \in \N\}$.
\end{exemple}

\begin{exemple}{}{Langage de Dyck}
    Le langage de Dyck (bon parenthésage) est engendré par~:
    \begin{itemize}
        \item $\Sigma = \{\code{(}, \code{)}\}$
        \item $S \rightarrow {(S)S}$
        \item $S \rightarrow \epsilon$
    \end{itemize}
\end{exemple}

\begin{remarque}{}{grammaires "non contextuelles" uniquement définies par les règles de production}
    En général, on donne une grammaire hors contexte uniquement avec les règles de production.\\
    On ne précise le symbole initial que si ce n'est pas $S$.\\
    Les symboles non terminaux sont exactement ceux que l'on rencontre à gauche des règles et les terminaux sont les autres qu'on rencontre.
\end{remarque}

\begin{remarque}{}{abréviation d'une famille de règles de production}
    Pour noter rapidement un ensemble de règles de production utilisant le même symbole non terminal comme départ~:
    $$\{\rp{X,$\alpha_1$},\, \dots,\, \rp{X}{$\alpha_k$}\} \subset P$$
    avec $X\in V$,\, on note~:
    $$X \rightarrow \alpha_1 \vert \cdots \vert \alpha_k$$
    Ce n'est qu'une notation, on a bien $k$ règles de production derrière ça.
\end{remarque}

\begin{exemple}{}{exploitant ce raccourci}
    $G_1$ s'écrit~:
    $$S_1 \rightarrow aS_1b \vert \epsilon$$
\end{exemple}

\subsection{Langages réguliers et langages hors contexte}

Un résultat à connaître et savoir redémontrer.
\begin{proposition}{}{régulier $\implies$ hors contexte}
    Soit $\Sigma$ un alphabet, Tout langage sur $\Sigma$ régulier est hors contexte.
\end{proposition}
Voir Démo 2 (elle est incomplète)

\begin{remarque}{}{hors contexte $\nRightarrow$ régulier}
    il existe des langages hors contexte qui ne sont pas réguliers. En effet on a montré que pour $G_1 = (\Sigma_1, V_1, P_1, S_1)$ définie par~:
    \begin{itemize}
        \item $\Sigma_1 = \{a,b\}$
        \item $V_1 = \{S_1\}$
        \item $P_1 = \{S_1 \rightarrow a S_1 b,\,S_1\rightarrow \epsilon\}$
    \end{itemize}
    Alors $\mc{L}_{G_1}(S_1) = \{a^nb^n,\, n \in \N\}$.\\\\
    Ce langage est donc hors contexte, mais n'est pas régulier.
\end{remarque}

\begin{definition}{}{grammaire hors contexte linéaire (HP)}
    Une grammaire $G = (\Sigma, V, P, S)$ hors contexte est dite \notion{linéaire} si~:
        $$P \subset V \times \big(\Sigma^* \cup \Sigma^*V\Sigma^*\big)$$
    Moralement, on a toujours au plus un symbole non terminal (voir Fig.1)
\end{definition}

\begin{definition}{}{grammaire hors contexte linéaire gauche (HP)}
    Une grammaire $G = (\Sigma, V, P, S)$ hors contexte est dite \notion{linéaire} si~:
        $$P \subset V \times \big( \Sigma^* \cup V \Sigma^* \big)$$
    Moralement, Le facteur "produit" commence par un unique symbole non terminal ou n'en a aucun.
\end{definition}

\begin{definition}{}{grammaire hors contexte linéaire droite (HP)}
    Une grammaire $G = (\Sigma, V, P, S)$ hors contexte est dite \notion{linéaire} si~:
        $$P \subset V \times \big( \Sigma^* \cup \Sigma^*V \big)$$
    Moralement, Le facteur "produit" termine par un unique symbole non terminal ou n'en a aucun.
\end{definition}

\begin{remarque}{}{chaine d'implications}

    linéaire droit implqiue linéaire implique hors contexte
    linéaire gauche implique linéaire implique hors contexte.
    
\end{remarque}

\begin{definition}{}{langages linéaire, linéaire droit, linéaire gauche}
    Un langage est dit linéaire (resp. simple, droit, gauche) lorsqu'il peut être engendré par une grammaire linéaire (resp. simple, linéaire droite).
\end{definition}

\begin{remarque}{}{existence de langages linéaires non réguliers}
    $$\code{S} \rightarrow \code{aSb} \vert \epsilon$$ est linéaire et engendre $\{a^n b^n,\, n \in \N\}$
\end{remarque}

\begin{proposition}{}{pour un langage, régulier $\ssi$ linéaire droit $\ssi$ linéaire gauche} 
    Soit $L$ un langage. Les propriétés suivantes sont équivalentes.
    \begin{enumeratebf}
        \item $L$ est régulier
        \item $L$ est linéaire droit
        \item $L$ est linéaire gauche
    \end{enumeratebf}
\end{proposition}

\begin{remarque}{}{sur la démo}
    En montrant régulier implqiue linéaire droit, on montre à nouveau que régulier implqiue hors contexte, ceci constitue une autre démo que la propriété précédente.
\end{remarque}

Démo 3.

\subsection{Système d'équations d'une grammaire}

\begin{definition}{}{}
    Soit $G = (\Sigma, V, P,S)$ une grammaire hors contexte avec $V = \{S, X_1, \dots, X_n\}$. Soit $L = (L_0,\, \dots,\, L_n)$ un $(n+1)$-uplet de langages sur $\Sigma$. On définit par induction~:
    \begin{itemize}
        \item $\epsilon(L) = \{\epsilon\}$ (base)
        \item $\forall a \in \Sigma,\, a(L) = \{a\}$ (base)
        \item $S(L) = L_0$ (base)
        \item $\forall i \in \intint{1}{n},\, X_i(L) = L_i$ (base)
        \item $\forall (\alpha,\beta) \in \Big((\Sigma \cup V)^*\Big)^2,\, \alpha\beta(L) = \alpha(L)\cdot \beta(L)$ (compatibilité avec la concaténation)
        \item $\forall K \subset (\Sigma \cup V)^*,\, K(L) = \bigcup_{w \in K} w(L)$ (compatibilité avec l'union)
    \end{itemize}
    Le \notion{système d'équations $\mc{S}(G)$ de la grammaire $G$} est le suivant~:
    $$\begin{cases*}
        \forall i \in \intint{1}{n},\, L_i = \bigcup_{(X_i,\alpha) \in P}\alpha(L)\\
        L_0 = \bigcup_{(S, \alpha) \in P} \alpha(L)
    \end{cases*}$$
\end{definition}

\begin{remarque}{}{}
    À chaque symbole on associe un langage $L_i$. On veut montrer que $\mc{L}_G = (\leng{G}{S},\, \leng{G}{X_1},\,\dots,\,  \leng{G}{X_n})$ est solution du système $\mc{S}(G)$
\end{remarque}

\begin{exemple}{}{}
    \begin{align*}
        G_1 :&\\ 
        &S \rightarrow aX_1b\\
        &X_1 \rightarrow cX_2d\\
        &X_2 \rightarrow \epsilon \vert S
    \end{align*}
    Avec cette grammaire, on obtient le système suivant~:
    $$\begin{cases*}
        L_0 = \{a\}L_1\{b\}\\
        L_1 = \{c\}L_2\{d\}\\
        L_2 = \{\epsilon\} \cup L_0
    \end{cases*}$$
\end{exemple}

\begin{remarque}{}{notation liées à un sytème d'équations d'une grammaire}
    On note $\mc{L}_G = (\leng{G}{S},\, \leng{G}{X_1},\,\dots,\,  \leng{G}{X_n})$ le $(n+1)$-uplet de langages engendrés par les symboles non terminaux.
\end{remarque}

\begin{proposition}{}{lemme 1 concernant ce $(n+1)$-uplet}
    $\forall \alpha \in (V \cup \Sigma)^*,\, \leng{G}{\alpha} = \alpha(\mc{L}_G)$
\end{proposition}
Démo 4

\begin{proposition}{}{Lemme 2 concernant ce ...}
    Soit $L$ un $(n+1)$-uplet de langages sur $\Sigma$ solution de $\mc{S}(G)$.\\
    $$\forall (\alpha,\beta) \in (\Sigma \cup V)^*,\, (\alpha \crt \beta) \implies (\beta(L) \subset \alpha(L))$$
\end{proposition}
demo 5

\begin{theoreme}{}{utilisant les deux lemmes}
    $\mc{L}_G = (\leng{G}{S},\, \leng{G}{X_1},\,\dots,\,  \leng{G}{X_n})$  est l'unique solution minimale pour l'inclusion composante par composannte de $\mc{S}(G)$.
\end{theoreme}
Démo 6

\begin{exemple}{}{pratique}
    En pratique on peut résoudre le système en utilisant le lemme d'Arden pour déterminer le langage engendré par une grammaire hors contexte pas trop complexe. D'après la théorie précédente, la solution sera l'unique solution minimale pour le système de grammaire.
    $\begin{align*}
        G : &S \rightarrow  SX_1 \vert \epsilon\\
        &X_1 \rightarrow aX_1 \vert b 
    \end{align*}$

    $$\begin{cases*}
        L_0 = L_0L_1 \cup \{\epsilon\}\\
        L_1 = \{a\}L_1\{b\}
    \end{cases*}$$
    Voir Démo 7
\end{exemple}

\begin{exemple}{}{pénible}
    Pour $S\rightarrow aSb \vert \epsilon$,
    On ne peut pas appliquer le lemme d'Arden, il faut faire une induction~:
    $$L = (A_1LA_2) \cup B$$
    Une solution minimale est $\{m_1 m_0 m_2,\, k \in \N,\, m_1 \in A_1^k,\, m_2 \in A_2^k,\, m_0 \in B\}$. La démo est celle du lemme d'Arden mais il ne couvre pas ce cas c'est tout.
\end{exemple}

\begin{remarque}{}{}
    Les systèmes d'équation de grammaire et le lemme d'Arden sont des outils HORS PROGRAMME. À l'écrit, il faut le redémontrer une fois avant d'utiliser le lemme d'Arden ou faire une induction structurelle dans les cas particuliers.\\
    Pour $\mc{S}(G)$, on justifie par une phrase en s'appuyant sur la première règle utilisée et on donne $\mc{S}(G)$.
\end{remarque}

\subsection{Liens avec les définitions inductives}

Les définitions des grammaires hors contexte sont très liées aux définitions inductives.

\subsection{Hiérarchie de Chomsky - HP, culture générale}

\begin{theoreme}{}{hiérarchie de Chomsky}
    Il s'agit d'un théorème qui définit une "hiérarchie" sur les langages~:
    \begin{enumeratebf}
        \item Tout langage régulier (rationnel, de type 3) est un langage linéaire (réciproque fausse)
        \item Tout langage linéaire est un langage hors contexte (de type 2). La réciproque est fausse.
        \item Tout langage hors contexte (de type 2) est un langage contextuel (de type 1). La réciproque est fausse
        \item Tout langage contextuel (de type 1) est un langage récursif (réciproque fausse). C'est à dire les langages récurssifs (\ie reconnus par une machine de Turing déterministe, sans calcul infini).
        \item Tout langage récursif est récursivement énumérable (= de type 0 d'après le théorème de Chomsky) (reconnaissable par une machine de Turing).
    \end{enumeratebf}
    Voir Fig.8
\end{theoreme}

\section{Arbre d'analyse}

\begin{definition}{}{arbre d'analyse}
    Un \notion{arbre d'analyse} (ou \notion{arbre d'analyse syntaxique}, ou \notion{arbre de dérivation}) pour une grammaire hors contexte $G = (\Sigma, V,P,S)$ et un mot $w \in \leng{G}{S}$ est un arbre~:
    \begin{itemize}
        \item dont les étiquettes sur les noeuds sont des symboles dans $\Sigma\cup V \cup\{\epsilon\}$~:
        \begin{itemize}
            \item $\Sigma \cup \{\epsilon\}$ pour les feuilles
            \item $V$ pour les noeuds internes (racine comprise)
        \end{itemize}
        \item dont la racine est étiquettée par $S$
        \item pour tout noeud interne $n$ d'étiquette $X \in V$, de fils d'étiquettes $x_1, \dots, x_k$, on a $(X,x_1,\dots x_k) \in P$
    \end{itemize}
    On appelle alors \notion{frontière} de l'arbre d'analyse le mot obtenu en concaténant les symboles des feuilles de gauche à droite : c'est $w$.
\end{definition}

\begin{exemple}{}{d'arbre d'analyse}
    Pour $G : S \rightarrow aSb \vert \epsilon$, Voir Fig.9
\end{exemple}


\begin{remarque}{}{propriété de l'arbre d'analyse}
    L'arbre de l'arbre d'analyse est la  preuve qu'un mot est engendré par une grammaire. Il rend compte de toutes les règles de production utilisées mais pas entièrement de l'ordre. On le verra au prochain cours.
\end{remarque}

\input{../../stock/pied.tex}