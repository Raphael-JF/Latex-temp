\input{../../stock/en-tete_v4.tex}
\begin{document}
\begin{adjustwidth}{-3cm}{-3cm}
\input{../../stock/commands.tex}
\newcounter{chapitre}
\setcounter{chapitre}{8}

\section{Fils d'exécution}

\begin{definition}{}{processus}
    Un \notion{processus} est un programme en cours d'exécution. Il occupe la mémoire selon l'organisation suivante~
    \begin{itemize}
        \item le \notion{segment de données}, qui constitue une zone de mémoire "fixe" pour stocker les variables globales, ainsi que les constantes.
        \item le \notion{tas}, où sont stockées les variables allouées dynamiquement.
        \item la \notion{pile}, une zone de mémoire de taille variable où sont stockés pour chaque \notion{fil d'exécution}~:
        \begin{itemize}
            \item le \notion{bloc d'activation} contenant les variables locales aux fonctions appelées par le fil.
            \item un \notion{pointeur d'instruction} vers le segment de code qui contient l'adresse de la prochaine instruction du fil dans le \notion{segment de code}.
        \end{itemize}
        \item le \notion{segment de code}, où sont stockés les instructions du programme.
    \end{itemize}
\end{definition}

\begin{remarque}{}{contexte d'un processus}
    Deux processus ne partagent pas leur zone de mémoire. Pour communiquer, ils doivent effectuer des \notion{appels système} (interaction avec du matériel physique comme la mémoire ou les périphériques) coûteux. Pour cette raison on dit que \notion{le contexte d'un processus est lourd}.
\end{remarque}

\begin{remarque}{}{}
    Pour les faire communiquer, on a besoin de faire des \notion{appels système}. Par exemple, on utilise un fichier utilisé par deux processus en lecture/écriture.
\end{remarque}

\begin{remarque}{}{}
    On dit que le \notion{contexte d'un processus} est \notion{lourd} car sa création est son activation sont coûteux.
\end{remarque}

\begin{definition}{}{fil d'exécution (thread)}
    Un \notion{fil d'exécution} (dit \textit{thread}) est une séquence d'\notion{instructions atomiques} : instructions s'effectuant individuellement sans interruption. On le définit par~:
    \begin{itemize}
        \item sa \notion{tâche}, sous la forme d'une fonction.
        \item une pile listant les intructions atomiques à traîter.
    \end{itemize}
    Un processus peut contenir plusieurs fils d'exécutions dont un désigné \notion{principal}.
\end{definition}

\begin{remarque}{}{création d'un fil d'exécution}
    Pour créer un fil d'exécution, on définit sa \notion{tâche} sous la forme d'une fonction et on lui associe une pile listant les instructions à traîter.
\end{remarque}

\begin{remarque}{}{}
    Un processus peut contenir plusieurs fils d'exécutions~:
    \begin{itemize}
        \item un désigné \notion{principal}
        \item tous ceux créés en plus : on a autant de piles dans la mémoire que de fils d'exécution.
    \end{itemize}
    voir figure 
\end{remarque}

\begin{remarque}{}{contexte d'un fil d'exécution}
    Les fils d'exécution peuvent partager des données stockées dans le tas ou le segment de données. Ainsi, on dit que le \notion{contexte} des fils d'exécution est \notion{léger} car la création, l'activation et la communication est facilitée et peu coûteuse.\\
    Pour autant, les fils d'exécution s'exécutent indépendamment les uns des autres.
\end{remarque}

Plusieurs fils d'exécution peuvent partager des données stockées dans le tas ou le segment de données.

Théoriquement, ils ont accès avec les adresses mémoires aux piles des autres fils, mais ce n'est pas utilisé en pratique.

On dit que le \notion{contexte} des fils d'exécution est \notion{léger} car la création, l'activation et la communication est facilitée et peu coûteuse.

Les fils d'exécutions s'exécutent indépendamment les uns les autes.

\begin{definition}{}{programmes séquentiel, concurrent}
    Un \notion{programme séquentiel} est un programme utilisant un seul fil d'exécution. Un \notion{programme concurrent} en utilise plusieurs à la fois.
\end{definition}

Un fil seul est \notion{séquentiel}. On dit qu'un programme est \notion{concurrent} lorsqu'il a plusieurs fils d'exécution (concurrent au sens de "se passe en même temps").

L'exécution repose sur un \notion{entrelacement} \notion{non déterministe} des fils d'exécution.

Dans un programme concurrent, initialement on a un fil d'exécution principal, puis on a des phases avec plusieurs fils d'execution en parallèle. Il faut explicitement attendre la terminaison des \notion{fils secondaires} pour revenir à un unique fil principal avant la fin du processus. Voir Fig.2 \\
Si on n'attend pas explicitement, des fils secondaires peuvent ne pas avoir terminé leur \notion{tâche} au moment où le fil principal termine le processus.

\begin{implementation}{exemple de programme concurrent en pseudocode}
    \code{t} pour tâche.
    \begin{lstLNat}
    $F$(nom):
        pour i=1 jusqu'à 10:
            afficher nom
            afficher i
    MAIN(): // programme concurrent
        t1 = création du fil 1 réalisant la tâche $F$ sur "Fil 1"
        t2 = création du fil 2 réalisant la tâche $F$ sur "Fil 2"
        attendre la fin de t1
        attendre la fin de t2
        afficher "Fin"
    \end{lstLNat}
    On a trois fils d'exécutions en parallèle~:
    \begin{itemize}
        \item un pour la fonction \code{MAIN} (le \notion{fil principal})
        \item le fil 1 pour $F$ sur "Fil1"
        \item le fil 2 pour $F$ sur "Fil2"
    \end{itemize}
\end{implementation}

On a plusieurs résultats possibles, issus de différents entrelacements.\\

À chaque exécution, on peut avoir une issue différente : c'est \notion{non déterministe}.



\begin{exemple}{}{}
    voir feuilles
\end{exemple}

\begin{exemple}{}{}
    voir feuilles
\end{exemple}

En C, pour pouvoir créer des fils d'exécution, il est nécessaire d'inclure l'en-tête~:
$$\code{\#include <pthread.h>}$$
permettant l'utilisation de la bibliothèque POSIX \code{pthread}.\\
De plus, à la compilation, il faut ajouter l'option de compilation~:
$$\code{-pthread}$$

On a un type décrivant les fils d'exécution~:
$$\code{pthread\_t}$$
\\\\\\\\\\
Pour créer un fil d'exécution, on utilise la fonction de prototype suivant~:
\begin{lstC}
    int pthread_create(
        pthread_t* thread,
        const pthread_attr_t* attr,
        void* (*start_routine)(void*), //transtypage
        void* arg
    );
\end{lstC}

Avant d'appeler \code{pthread\_create}, il faut créer un pointeur du type \code{pthread\_t*} valide (la mémoire est allouée avant appel, mais avoir défini sur la pile convient, auquel cas on y rentre \code{\&file}, car il lui faut quand même un pointeur, cf exemple ci-dessous).\\
On déclare un fil~:
\begin{lstC}
    pthread_t t1; // alloué sur la pile ici
    pthread_create(&t1, $\dots$); // et on y met le pointeur
\end{lstC}



\begin{remarque}{}{concernant le paramètre \code{*start\_routine}}
    On écrit préalablement une fonction \code{start\_routine} qui correspond à la tâche du fil créé et qui doit être du prototype~:
    \begin{lstC}
        void* start_routine(void* arg)
    \end{lstC}
    Bien qu'on ait un unique argument de type \code{void*}, grâce au transtypage et à l'utilisation de structures bien choisies, on peut pallier à cette restriction. De même pour le retour de la fonction $\code{start\_routine}$.\\
    Néammoins, en MPI, on ne récupère pas la sortie : en pratique, on écrit dans des variables partagées entre les fils (merci les pointeurs !).\\
    Théoriquement, la fonction prend un pointeur vers une fonction, mais les deux syntaxes fonctionnent (attention au sujet dans ce cas).
    \begin{lstC}
        pthread_create(&t1, $\dots$, start_routine, $\dots$);
        pthread_create(&t1, $\dots$, &start_routine, $\dots$);
    \end{lstC}
\end{remarque}

\begin{remarque}{}{concernant le paramètre \code{void* arg}}
    \code{arg} est l'argument que l'on donne à la fonction \code{start\_routine} pour le fil créé.\\
    Il vaut \code{NULL} si aucun argument n'est à transférer.
\end{remarque}

\begin{remarque}{}{concernant le paramètre \code{const pthread\_attr\_t* attr}}
    En MPI, il vaut \code{NULL}.
\end{remarque}

\begin{remarque}{}{concernant la sortie de \code{pthread\_create}}
    La valeur de retour de \code{pthread\_create} est un entier~:    
    \begin{itemize}
        \item \code{0} si tout se passe bien.
        \item un code d'erreur sinon.
    \end{itemize}
    Mais en MPI, on ignore cette sortie.
\end{remarque}

Pour attendre la fin de l'exécution d'un fil dans un autre, (pas forcément dans le \notion{fil principal}), on utilise la fonction~:\\
\begin{lstC}
    int pthread_join(
        pthread_t thread,
        void** value_ptr
    );
\end{lstC}

\begin{remarque}{}{paramètre \code{thread}}
    \code{thread} est le fil dont on attend la terminaison. En MPI, \code{value\_ptr} est toujours \code{NULL}.
\end{remarque}

\begin{definition}{}{synchronisation, et pratique}
    La \notion{synchronisation} permet de coordonner l'exécutions des fils d'exécution afin qu'ils puissent travailler ensemble de façon cohérente.\\
    En C comme en OCaml, la fonction de synchronisation permet de bloquer l'exécution de la fonction principale (dans laquelle la fonction est appelée) pour finir l'exécution du fil d'exécution passé en argument.
\end{definition}


\begin{implementation}{implémentation en C d'une démonstration de l'entrelacement non déterministe}
    \begin{lstC}
        #include <pthread.h>
        #include <stdio.h>

        void* F(void* arg){ // au lieu de char* str, on GÉNÉRALISE en y 
        // mettant directement le arg qu'on rentre dans la création du thread !
            char* chaine = (char*) arg;
            for (int i=0; i<1000; i++){
                printf("%s %d\n", chaine, i);
            }
            return NULL; // important !
        }

        int main(){
            pthread_t t1, t2;
            pthread_create(&t1, NULL, &F, "Fil1");
            pthread_create(&t2, NULL, &F, "Fil2");
            pthread_join(t1, NULL);
            pthread_join(t2, NULL);
            printf("Fin");
            return 0;
        }
    \end{lstC}
\end{implementation}

En OCaml, on utilise le module \code{Thread}.
Cela ne fonctionne pas en mode interactif, il faut compiler-exécuter avec des options de compilation.
$$\code{ocamlc -I +threads unix.cma threads.cma ficher.ml -o fichier}$$
$$\code{ocamlopt -I +threads unix.cmxa threads.cmxa ficher.ml -o fichier}$$
Le type des fils d'exécution est~:
$$\code{Thread.t}$$
Pour créer un fil d'exécution~:
\begin{lstOCaml}
    Thread.create : ('a -> 'b) -> 'a -> Thread.t
\end{lstOCaml}
\code{Thread.create f x} crée et "renvoie" un fil d'exécution que réalise la fonction \code{f} sur l'argument \code{x}.\\\\
Pour attendre la terminaison d'un fil dans un autre~:
\begin{lstOCaml}
    Thread.join : Thread.t -> unit
\end{lstOCaml}

\begin{implementation}{implémentation en OCaml d'une démonstration de l'entrelacement non déterministe}
    \begin{lstOCaml}
        let f chaine = 
            for i=0 to 1000 do
                print_string (chaine^(string_of_int i)^"\n");
            done

        let t1 = Thread.create f "File 1 : "
        let t2 = Thread.create f "File 2 : "
        let () = Thread.join t1; Thread.join t2
        let () = print_string "Fin"
    \end{lstOCaml}
\end{implementation}

\section{Synchronisation et concurrence}

\subsection{Pourquoi faut-il synchroniser ?}

Pour que les fils coopèrent sur des variables partagées, il faut leur permettre de communiquer.

\begin{implementation}{deux fils d'exécutions communiquantes en C}
    On écrit un programme qui utilisent deux fils d'exécution qui incrémentent un \notion{entier commun}.
    \begin{lstC}
        void* f(void* arg){
            for (int i=0; i<1000; i++){
                *((int*) arg) = *((int*) arg) + 1; // déréférencement
            }
            return NULL;
        }
        int main(){
            int x = 0;
            pthread_t t1, t2;
            pthread_create(&t1, NULL, &f, (void*) &x);
            pthread_create(&t2, NULL, &f, (void*) &x); // l'entier x est une varible partagée

            pthread_join(t1, NULL); // demande d'attendre t1
            pthread_join(t2, NULL); // demande d'attendre t2
            printf("x = %i \n",x);
            return 0;
        }
    \end{lstC}
    On s'attend à l'affichage de $\code{"x = 2000"}$, mais en pratique c'est plus petit.
\end{implementation}
    

l'incrémentation n'est pas une instance atomique~:
\begin{itemize}
    \item on a une phase de récupération de la valeur
    \item on a une phase d'incrémentation locale de la valeur récupérée (espace mémoire temporaire dédié)
    \item on a une dernière phase où l'on écrit la nouvelle valeur dans l'espace mémoire dédié.
\end{itemize}
Dans l'entrelacement, le fil actif peut changer entre deux phases. Donc la valeur écrite dans la dernière phase peut être inférieure à la valeur. Voir Fig.3

\subsection{Gestion des sections critiques}

\begin{definition}{}{section critique (d'après le cours)}
    Une \notion{section critique} est une suite d'instructions d'un processus nécessitant d'accéder à une ressource unique et commune à plusieurs fils d'exécution. On parle de \notion{variable partagée}.
\end{definition}

\begin{definition}{}{section critique}
    Une \notion{section critique} est une suite d'instructions que jamais plus d'un thread ne doit exécuter simultanément.\\
    Lorsqu'un fil accède à une section critique, on dit qu'il entre en \notion{exclusion mutuelle} sur la ressource~: il empêche tous les autres d'y accéder.
\end{definition}

\textbf{Enjeu} : Un seul fil doit être capable d'accéder à une section critique pour une même ressource, en lecture ou en écriture. \\\\

Pour assurer cela, on peut protéger la ressource partagée avec un \notion{verrou} que l'on peut sceller ou lever pour limiter l'accès aux sections critiques dans les différents fils. On dit qu'un fil qui est le seul à accéder à une section critique est en exclusion mutuelle.

\begin{definition}{}{exclusion mutuelle}
    Lorsqu'un fil accède à une section critique, on dit qu'il entre en \notion{exclusion mutuelle} sur la ressource : il empêche tous les autres d'y accéder.
\end{definition}

\begin{definition}{}{course critique}
    Lorsque deux fils cherchent à accéder à une même ressource et que plusieurs entrelacements donnent une issue différente, on parle de \notion{course critique}.
\end{definition}

Lorsque deux fils cherchent à accéder à une même ressources et que deux entrelacements au moins donnent deux résultats différents, on parle de \notion{course critique}.

\begin{remarque}{}{concernant l'exemple de l'incrémentation mutuelle}
    Dans l'exemple de la section 2.1, on obtient différentes valeurs pour la variable commune : il s'agit d'une course critique.\\
    On cherche à éviter les courses critiques car elles causent un comportement non déterministe.
\end{remarque}

\begin{definition}{}{propriétés usuelles d'un programme concurrent}
    Pour un programme concurrent, on cherche à garantir plusieurs propriétés~:
    \begin{enumeratebf}
        \item \notion{sûreté (ou principe de l'exclusion mutuelle)} : On a au plus un fil d'exécution accédant une section critique pour une même ressource au même moment.
        \item \notion{vivacité (ou absence de famine)}~: tout fil d'exécution demandeur d'accès à une ressource partagée aura à un moment donné accès à la ressource.
        \item \notion{absence d'inter-blocage}~: lorsque plusieurs fils demandent accès à une ressource partagée simultanément, au moins un obtient l'accès.
    \end{enumeratebf}
\end{definition}
\textbf{N.B} : la troisième est moins forte (difficile à réaliser) que la deuxième (2 implique 3), on la met car parfois on veut juste vérifier la troisième.\\

\begin{remarque}{}{atomicité d'une section critique}
    L'atomicité d'une section critique est importante. La mise en place de verrou de la forme suivante permet d'induire une atomicité de la section critique~:
    \begin{lstLNat}
        lock()
        // section critique
        unlock()
    \end{lstLNat}
\end{remarque}

\begin{exemple}{}{de verrouillage}
    \begin{center}
        \begin{tabular}{c|c}
            \textbf{Fil 1} & \textbf{Fil 2} \\
            \code{t = x}         & \code{u = x}   \\ 
            \code{x = t+1}         & \code{x = u+2}          \\ 
        \end{tabular}
    \end{center}
    La variable \code{x} est une ressource partagée. Les deux morceaux sont des \notion{sections critiques pour la ressource \code{x}}.\\
    Dans l'état actuel, on a une course critique car les deux entrelacements donnent des résutlats différents~:
    \begin{center}
        \begin{tabular}{c|c}
            \textbf{Fil 1} & \textbf{Fil 2} \\
            \code{t = x}         &    \\ 
            \code{x = t+1}         &         \\ 
            & \code{u = x}  \\
            & \code{x = u+2}  
        \end{tabular}
    \end{center}
    La valeur de la variable \code{x} est augementée de 3.
    \begin{center}
        \begin{tabular}{c|c}
            \textbf{Fil 1} & \textbf{Fil 2} \\
            \code{t = x}         &    \\ 
                     &   \code{u = x}     \\ 
            &   \code{x = u+2} \\
            \code{x = t+1} &   
        \end{tabular}
    \end{center}
    La valeur de la variable \code{x} est incrémentée.
\end{exemple}

\begin{remarque}{}{trace d'exécution}
    On peut parler de \notion{trace d'exécution} lorsque l'on donne un entrelacement possible.
\end{remarque}

\begin{exemple}{}{élimination de section critique}
    Pour ne plus avoir de section critique, on met en place un verrou \code{m}.
    \begin{center}
        \begin{tabular}{c|c}
            \textbf{Fil 1} & \textbf{Fil 2} \\
            \code{lock(m)} & \code{lock(m)} \\
            \code{t = x} & \code{u = x}\\
            \code{x = t+1} & \code{x = u+2}\\
            \code{unlock(m)} & \code{unlock(m)}
        \end{tabular}
    \end{center}
    Le premier fil à lire l'instruction \code{lock(m)} verrouille \code{m}, le second attend alors que \code{m} soit déverrouillé pour lire la prochaine instruction.
\end{exemple}

\begin{definition}{}{mutex}
    On met en place des verrous à l'aide de \notion{mutex} qui disposent de deux opérations élémentaires~:
    \begin{itemize}
        \item une opération de \notion{verrouillage}~: verrouille le mutex s'il est déverrouillé, ou bien attend que le mutex soit déverrouillé puis le verrouille.
        \item une opération de \notion{déverrouillage}~: déverrouille le mutex.
    \end{itemize}
    On dit qu'un mutex est une \notion{primitive de synchronisation}, car c'est un outil utilisant des instructions simples pour gérer la synchronisation des fils d'exécutions.
\end{definition}

\begin{remarque}{}{usage de mutex en pratique}
    Dans certains langages (dont OCaml), un mutex doit être verrouillé puis déverrouillé dans un même fil. Cela reste une bonne pratique dans les autres langages.\\
    Lorsqu'on souhaite verrouiller dans un fil et déverrouiller dans un autre, on préfèrera l'utilisation de \notion{sémaphores}~: une généralisation des mutex.
\end{remarque}

\begin{definition}{}{sémaphores binaire, à compteur}
    Il existe deux types de \notion{sémaphores}~:
    \begin{itemize}
        \item les \notion{sémaphores binaires} : c'est comme des mutex, mais qui fonctionnent entre plusieurs threads
        \item les \notion{sémaphores à compteur} : une sutructre de données constituée d'un \notion{compteur} et d'une \notion{file d'attente}.
    \end{itemize}
    On initialise un sémaphore à compteur avec une file d'attente vide et un compteur initialisé à un entier positif.\\
    Le sémaphore à compteur exploite deux opérations élémentaires~:
    \begin{itemize}
        \item \notion{demande d'accès}~: notée \code{P} ou bien \code{down} (\code{wait} en C, \code{acquire} en OCaml).\\
            On vérifie que le compteur est strictement positif~
            \begin{itemize}
                \item si oui, one le décrémente et on passe à l'instruction suivante.
                \item sinon, on attend que le compteur soit incrémenté pour un autre fil et lui applique le cas précédent dès que ça arrive.
            \end{itemize}
            pendant qu'il attend, le fil d'exécution est placé dans la file d'attente du sémphore à compteur.
            \item \notion{fin d'accès}~: notée \code{V} ou bien \code{vp} (\code{post} en C, \code{release} en OCaml)\\
            Le compteur est incrémenté et si la file d'attente n'est pas vide, un des fils de la file d'attente du sémaphore (à compteur) est "réveillé" pour décrémenter le compteur et poursuivre l'exécution du dit fil. \textbf{les fils sont donc réveillés dans un ordre non déterministe}.
    \end{itemize}
    \textit{on fera un pseudocode de fonctionnement, la FC est lourde là}
\end{definition}

\begin{definition}{}{initialisation d'un sémaphore à compteur}
    On initialise un sémaphore à compteur avec~:
    \begin{itemize}
        \item une \notion{file d'attente} vide, qui stockera les fils concernés par le sémaphore.
        \item un \notion{compteur}, entier naturel non nul désignant le nombre maximal de fils pouvant accéder à la section critique protégée par le sémaphore.
    \end{itemize}
\end{definition}

\begin{implementation}{demande d'accès à un sémaphore}
    On note \code{s} le sémaphore. le fil \code{fil} est celui dans lequel \code{P} est appelée.
    \begin{lstLNat}
    P(s, fil):
        si s.compteur == 0: 
            // s : "Je ne peux pas vous donner l'accès, je garde votre nom."
            ajouter file à s.file
            tant que compteur == 0:
                Attendre // fil : "Ok, j'attends."
        s.compteur = s.compteur - 1 // un emplacement libre de moins
    \end{lstLNat}
\end{implementation}

\begin{implementation}{annonce de fin d'accès à un sémaphore}
    On note $\mc{S}$ le sémaphore.
    \begin{lstLNat}
    V($\mc{S}$):
        si $\mc{S}$.file n'est pas vide:
            fil = defiler($\mc{S}$.file) // $\mc{S}$ : "C'est à vous, $\code{fil}$."
            réveiller(fil) // $\code{fil}$ : "Entendu, c'est reparti."
        $\mc{S}$.compteur = $\mc{S}$.compteur + 1 // un emplacement de libéré
    \end{lstLNat}
\end{implementation}

\begin{remarque}{}{rôle du compteur du sémaphore à compteur}
    Le compteur joue un rôle de \notion{compteur de ressources}.\\
    La demande d'accès correspond à la récupération d'une ressource pour passer à la suite~: on attend s'il n'y en a pas de disponible.\\
    La fin d'accès correspond à la situation où l'on rend disponible une ressource.
\end{remarque}

\begin{remarque}{}{comparaison entre sémaphores à compteur et mutex}
    Les sémaphores sont moins restrictifs que les mutexs : on peut avoir des fils qui font uniquement de la demande d'accès et des fils qui font uniquement de la fin d'accès (production de ressources).
\end{remarque}

\begin{remarque}{}{initialisation du compteur d'un sémaphore à compteur}
    On peut voir dans certaines situations la valeur du compteur initiale comme le nombre maximum souhaité de fils qui peuvent exploiter les ressources partagées simultanément.
\end{remarque}

\subsection{Mutex et sémaphores à compteurs en C}

L'utilisation des mutex se fait à l'aide de la bibliothèque \code{pthread.h}. Le type utilise pour les mutex est~:
$$\code{pthread\_mutex\_t}$$
On iniatialise un mutex déverrouillé de la façon suivante (pour l'avoir verrouillé direct, on le verrouille aussitôt après)~:
\begin{lstC}
    pthrad_mutex_t m = PTHREAD_MUTEX_INITIALIZER;
\end{lstC}
Si la variable protégée est une variable globale, le mutex doit être aussi une variable globale (définie sur le segment de données).
On ensuite trois fonctions à connaître~:
\begin{itemize}
    \item \notion{verrouillage}~:
    \begin{lstC}
        int pthread_mutex_lock(pthread_mutex_t* mutex);
    \end{lstC}
    L'instruction \code{pthread\_mutex\_lock(\&m)} verrouille \code{m} et renvoie 0 si tout s'est bien passé.
    \item \notion{déverrouillage}~:
    \begin{lstC}
        int pthread_mutex_unlock(pthread_mutex_t* mutex);
    \end{lstC}
    L'instruction \code{pthread\_mutex\_unlock(\&m)} déverrouille \code{m} et renvoie 0 si tout s'est bien passé.
    \item \notion{destruction}~:
    \begin{lstC}
        int pthread_mutex_destroy(pthread_mutex_t* mutex)
    \end{lstC}
    L'instruction \code{pthread\_mutex\_destroy(\&m)} détruit \code{m} (il ne peut plus être utilisé) et renvoie 0 si tout s'est bien passé.
\end{itemize}

L'utilisation des sémaphores demande l'inclusion du fichier d'en-tête \code{semaphore.h}~:
\begin{lstC}
    #include <semaphore.h>
\end{lstC}
Le type utilisé pour les sémaphores~: est \code{sem\_t}. On a ensuite quatre fonctions à connaître~:
\begin{enumeratebf}
    \item \notion{Initialisation}~:
    \begin{lstC}
    int sem_init(
        sem_t* sem,
        int pshared,
        unsigned int value,
    );
    \end{lstC}
    On déclare préalablement notre sémaphore et on l'initialise~:
    \begin{lstC}
    sem_t s;
    sem_init(&s, 0, nmb)
    \end{lstC}
    avec \code{nmb} la valeur positive que l'on veut pour le compteur initialement (peut être 0).\\
    \code{pshared} vaut 0 pour signifier que tous les fils partagent les sémaphores. Tout autre valeur relèverait du hors programme.\\
    La valeur 0 est renvoyée si tout se passe bien.
    \item \notion{destruction}~:
    \begin{lstC}
    int sem_destroy(sem_t* sem);
    \end{lstC}
    \item \notion{demande d'accès (P, attente)}~: 
    \begin{lstC}
    int sem_wait(sem_t* sem);
    \end{lstC}
    \item \notion{fin d'accès (V, libération)}~:
    \begin{lstC}
    int sem_post(sem_t* sem);
    \end{lstC}
\end{enumeratebf}

\subsection{Mutex et sémaphores en OCaml}
Pour les mutex, on utilise en OCaml le module Mutex. Le type utilsé est \code{Mutex.t}. Trois fonctions à connaître~:
\begin{enumeratebf}
    \item \notion{création}~:
    \begin{lstOCaml}
    Mutex.create : unit -> Mutex.t
    \end{lstOCaml}
    \begin{lstOCaml}
    let m = Mutex.create ()
    \end{lstOCaml}
    \code{m} est un mute initialement déverrouillé
    \item \notion{verrouillage}~:
    \begin{lstOCaml}
    Mutex.lock : Mutex.t -> unit
    \end{lstOCaml}
    \code{Mutex.lock m} verrouille le mutex \code{m}
    \item \notion{déverrouillage}~:
    \begin{lstOCaml}
    Mutex.unlock : Mutex.t -> unit
    \end{lstOCaml}
    \code{Mutex.unlock m} déverrouille le mutex \code{m}.   
\end{enumeratebf}
L'utilisation des sémaphores en OCaml, n'appraît pas dans le programme mais se fait à l'aide du module \code{Semaphore} et des sous-modules \code{Semaphore.Binary} (osef, voir \code{Mutex} c'est pareil pour nous), et \code{Semaphore.Counting} (sémaphores à compteur).\\
Le type pour les sémaphores à compteur est \code{Semaphore.Counting.t}. On a trois fonctions à connaîtres~:
\begin{enumeratebf}
    \item \notion{Initialisation}~:
    \begin{lstOCaml}
    Semaphore.Counting.make : int -> Semaphore.Counting.t
    \end{lstOCaml}
    L'appel \code{Semaphore.counting.make c} crée un sémaphore à compteur de compteur initial égal à \code{c} un entier naturel.
    \item \notion{Demande d'accès : \code{P}}~:
    \begin{lstOCaml}
    Semaphore.Counting.acquire : Semaphore.Counting.t -> unit
    \end{lstOCaml}
    \item \notion{Fin d'accès : \code{V}}~:
    \begin{lstOCaml}
    Semaphore.Counting.release : Semaphore.Counting.t -> unit
    \end{lstOCaml}
\end{enumeratebf}

\subsection{Algorithme de Peterson}

L'algorithme de Peterson est une façon d'implémenter un mutex partagé uniquement par \textbf{2} fils d'exécution.\\
Cet algorithme utilise l'\notion{attente active} : lorsqu'un fil \code{A} demande l'accès à une section critique, si l'autre fil \code{B} est déjà sur sa section critique, le fil demandeur \code{A} vérifie "en boucle" s'il a le droit de continuer.\\
C'est très coûteux et peu efficace, mais c'est la première idée mise en place historiquement pour gérer la synchronisation. Aujourd'hui, il est possible de mettre un fil en attente passive et le réveiller, mais c'est HP.

\begin{remarque}{}{concernant l'algorithme}
    un booléen suffit ne suffit pas : les fils seraient en course critique.\\
    On va utiliser deux booléens et un entier~:
    \begin{itemize}
        \item un booléen par fil qui indique si chaque fil est demandeur. Il reste \code{Vrai} si le fil est dans la section critique.
        \item un entier qui informe sur le fil prioritaire parmi les deux mis en jeu : \code{0} pour le fil \code{A} et \code{1} pour le fil \code{B}. On dit que ce fil prioritaire \code{a les droits d'accès}
    \end{itemize}
\end{remarque}

\begin{implementation}{algorithme de Peterson de fonctionnement d'un mutex}
    L'algorithme de Peterson est une façon d'implémenter un mutex partagé uniquement par 2 fils d'exécution.
    \begin{lstLNat}
    flag = [false, false]  // indique si i veut accéder
    tour = 0               // indique qui a les droits d'accès (prio)
    lock(i):               // i est l'indice du fil.
        flag[i] = true
        tour = 1-i         // même si i veut accéder, il est poli
        tant que flag[1-i] et tour == 1-i: // i-1 attend et a la prio
            attendre       // on fait de l'attente active
    
    unlock(i):
        flag[i] = false
    \end{lstLNat}
\end{implementation}

\begin{remarque}{}{concernant l'algorithme}
    \begin{itemize}
        \item Lorsqu'un seul fil est demandeur, il n'attend pas car \code{flag[1-i]} est toujours valant \code{false}.
        \item Lorsqu'un fil est demandeur pendant que l'autre fil est dans la section critique. Il actualise son \code{flag} (son drapeau) et donne les droits d'accès à l'autre donc il attend jusqu'à que l'autre fil appelle \code{unlock}.
        \item Lorsque les deux fils demandent simultanément, ces deux drapeaux sont actualisés à \code{true} et \code{tour} prend la valeur $0$ ou $1$ selon l'entrelacement. Un seul des fils, celui qui a les droits d'accès, n'attend pas.
        \item \begin{itemize}
            \item Le \notion{principe d'exclusion mutuelle} est garanti : on n'a jamais les deux fils en même temps accédant à la section critique.
            \item Avec une bonne utilisation de \code{lock} et \code{unlock}, à savoir un \code{lock} toujours suivi d'un \code{unlock}. On a \notion{absence d'interblocage} : au moins un des deux fils a les droits d'accès.
            \item Avec une bonne utilisation, on a également \code{absence de famine}. car le fil qui attend obtiendra les droits d'accès si l'autre fil est de nouveau demandeur après sa première fin d'accès.
        \end{itemize}
    \end{itemize}
\end{remarque}

\begin{remarque}{}{correction (caractère thread-safe) de l'algorithme de Peterson}
    \begin{enumeratebf}
        \item Le \notion{principe d'exclusion mutuelle} est garanti (on n'a jamais les deux fils en même temps accédant à la section critique). En effet, lorsque les deux fils demandent simultanément, ces deux drapeaux sont actualisés à \code{true} et \code{tour} prend la valeur $0$ ou $1$ selon l'entrelacement. Un seul des fils accède à la section critique.
        \item L'\notion{absence de famine} est également respectée (un fil n'attend jamais indéfiniment pour l'accès) car si un fil est demandeur pendant que l'autre fil est dans la section critique, il actualise son \code{flag} et donne les droits d'accès à l'autre (il attend jusqu'à ce que l'autre fil appelle \code{unlock}).
        \item L'\notion{absence d'interblocage} (en cas de conflit, au moins un fil accède) est aussi vérifiée~: Si un fil quitte la section critique (\code{flag[i] = false}), l’autre fil peut progresser.
    \end{enumeratebf}
\end{remarque}

\subsection{Algorithme de la boulangerie (ou du BK) de Lamport}

L'algorithme de la boulangerie de Lamport est une façon d'implémenter un mutex partagé par $N\in \N$ fil d'exécutions. $N$ est fixé une fois pour toutes.
On utilise un principe de file d'attente, voire de file de priorité.\\
La demande d'accès se fait en deux étapes~:
\begin{enumeratebf}
    \item la \notion{phase d'acquisition} d'un numéro par le fil demandeur. Il prend (comme au BK) le numéro incrémenté du maximum des numéros déjà attribués.
    \item la \notion{phase d'attente active} pendant laquelle un fil vérifie, "en boucle" si c'est son tour \ie si c'est celui avec le plus petit numéro.\\
    En cas d'égalité (demande d'accès simultanée), on compare les indices (différents du numéro !) des fils (dans $\intint{0}{N-1}$).\\
    Attention, on utilise un marqueur booléen pour signaler qu'un fil est en \notion{phase d'acquisition} pour attendre avant de comparer les numéros.
\end{enumeratebf}

\begin{remarque}{}{}
    Il y a beaucoup d'attente active, ce qui est coûteux et ralentit le code. Il faut retenir l'idée. Cet algorithme n'est plus utilisé tel quel.
\end{remarque}

\begin{implementation}{algorithme de la boulangerie de Lamport pour un mutex partagé par $N$ fils}
    Ici, \code{i} est l'indice du fil : \code{0 <= i <= $N-1$}
    \begin{lstLNat}
    acqui = [false]*$N$ // si le fil i demande un ticket
    ticket = [0]*$N$ // 0 -> pas de ticket assigné

    lock(i):
        // Phase d'acquisition
        acqui[i] = true
        num = 0
        pour k = 0 jusqu'à $N-1$:
            num = max(num, ticket[k])
        ticket[i] = num + 1
        acqui[i] = false

        // Phase d'attente active
        pour k = 0 jusqu'à i-1:
            tant que acqui[k] ou (ticket[k] != 0 et ticket[k] <= ticket[i]):
                attendre
            // ok si k ne demande pas de ticket et 
            // k n'a pas de ticket ou un moins prioritaire=petit
        pour k = i+1 jusqu'à $N-1$:
            tant que acqui[k] ou (ticket[k] != 0 et ticket[k] < ticket[i]):
                attendre
    
    unlock(i):
        ticket[i] = 0
    \end{lstLNat}
\end{implementation}

\begin{remarque}{}{}
    \begin{itemize}
        \item Pendant la phase d'attente active, seul le fil d'exécution de plus petite (\code{ticket[i]}, \code{i}) n'attend pas.\\
        De plus, on attend si un fil est en cours d'acquisition au cas où il a récupéré un plus petit numéro mais ne l'a pas encore écrit.
        \item Une fois que le fil \code{i} a vérifié un fil \code{j} par rapport à \code{i}, il ne peut acquérir qu'un plus grand numéro que \code{i} car il regardera \code{ticket[i]} dans la phase d'acquisition.
    \end{itemize}
\end{remarque}

\subsection{Exmemple de situation d'interblocage avec une mauvaise situation des mutexs}

On écrit un ptit programme en OCaml~:
\begin{lstOCaml}
    let m1 = Mutex.create ()
    let m2 = Mutex.create ()

    let f1 () = 
        Mutex.lock m1;
        Mutex.lock m2;
        print_string "Section critique\n";
        Mutex.unlock m2;
        Mutex.unlock m1

    let f2 () = 
        Mutex.lock m2;
        Mutex.lock m1;
        print_string "Autre section critique\n";
        Mutex.unlock m1;
        Mutex.unlock m2
    
    let t1 = Thread.create f1 ()
    let t2 = Thread.create f2 ()
    let () = Thread.join t1; Thread.join t2
\end{lstOCaml}

Voici une trace d'exécution (un entrelacement) menant à un interblocage~:
\begin{lstLNat}
    t1 : Mutex.lock m1 (*m1 est bloqué*)
    t2 : Mutex.lock m2 (*m2 bloqué*)
    t1 : Mutex.lock m2 (*t1 est bloqué car il attend de pouvoir bloquer m2*)
    t2 : Mutex.lock m1 (*t2 est bloqué car il attend de pouvoir bloquer m1*)
\end{lstLNat}

\subsection{Petit complément HP}

Pour formaliser l'étude du comportement des mutex et des sémaphores partagés par deux fils d'exécution, on peut construire un \notion{diagramme de transition}, il s'agit d'un tableau à double entrée~:
\begin{itemize}
    \item les lignes correspondent aux instructions des fils d'exécution.
    \item les colonnes correspondent aux instructions de l'autre.
    \item dans les cases, on représente à l'aide d'entiers l'état des mutex et des sémaphores. 
    \begin{itemize}
        \item Pour un mutex~:
        \begin{itemize}
            \item 1 si c'est déverrouillé
            \item 0 si c'est verrouillé
            \item une valeur négative si c'est impossible, on a un blocage
        \end{itemize}
        \item pour un sémaphore, on utilsie la valeur du compteur comme s'il pouvait être négatif : une valeur négative indique un blocage.
    \end{itemize}
\end{itemize}
Pour l'exemple $(i_1, i_2)$ état de $(\code{m1}, \code{m2})$~:
\textit{à rattraper}
\section{Applications et schémas de synchronisation courants}
Au concours, plusieurs types d'exercices peuvent être poséés sur ce chapitre~:
\begin{enumeratebf}
    \item implémentation des mutex (algorithmes de Peterson et de la boulangerie de Lamport) et sémaphores (plus guidé car aucun algo n'est exigé au programme)
    \item comme dans le sujet 0 des mines : utilisation de mutex (parfois sémaphores) pour paralléliser les calculs.
    \item exercices théoriques de conception de schémas de synchronisation exploitant mutex et sémaphores pour répondre à une situation abstraite décrite : souvent du pseudo-code. On la travaille spécifiquement lors de cours n°4 du mardi 21/01/2024.
\end{enumeratebf}

\subsection{Attendre la fin d'une exécution.}
On considère une situaton impliquant deux fils d'exécution réalisant des tâches en deux phases. On pose $s$ un sémaphore de compteur initialement nul et on a l'entrelacement suivant~:
\begin{center}
    \begin{tabular}{c|c}
        \textbf{Fil 1} & \textbf{Fil 2}\\
        Phase 1 & Phase 1'\\
        \code{post(s)}& \code{wait(s)}\\
        Phase 2 & Phase 2'
    \end{tabular}
\end{center}
On veut que le \textbf{Fil 2} ne commence pas la Phase 2' avant que le fil 1 n'ait fini la phase 1.

\begin{implementation}{attendre la fin d'une exécution}
    \begin{lstC}
    sem_t attente; // on peut pas faire le sem_init ici !

    void* f1 (void* arg){
        printf("Début phase 1\n");
        for(int i = 0; i < 1000; i++){
            ;
        }
        printf("Fin de phase 1\n");
        sem_post(&attente);
        printf("Début de phase 2\n");
        for(int i = 0; i<1000; i++){
            ;
        }
        printf("Fin phase 2\n");
        return NULL;
    }

    void* f2 (void* arg){
        printf("Début phase 1'\n");
        for(int i = 0; i < 1000; i++){
            ;
        }
        printf("Fin de phase 1'\n");
        sem_wait(&attente);
        printf("début de phase 2'\n");
        for(int i = 0; i<1000; i++){
            ;
        }
        printf("Fin phase 2'\n");
        return NULL;
    }

    int main(){
        sem_init(&attente, 0, 0);
        pthread_t t1,t2;
        pthread_create(&t1, NULL, &f1, NULL);
        pthread_create(&t2, NULL, &f2, NULL);
        pthread_join(t1, NULL);
        pthread_join(t2, NULL);
        sem_destroy(&attente);
        return 0;
    }
    \end{lstC}
\end{implementation}

\subsection{Rendez-vous ou barrière de synchronisation}

On considère une situation où plusieurs fils doivent réaliser deux phases d'une tâche~:
\begin{center}
    \begin{tabular}{c}
        \textbf{Fil}\\
        Phase 1\\
        \code{--barrière de synchronisation--}\\
        Phase 2
    \end{tabular}   
\end{center}

On souhaite qu'aucun des fils n'entame la phase 2 avant que tous les fils aient terminé la phase 1.

\begin{implementation}{Rendez-vous ou barrière de synchronisation}
    \begin{lstC}
    #define N 20
    
    int compteur = 0;
    pthread_mutex_t m_compteur = PTHREAD_MUTEX_INITIALIZER;
    sem_t rendez_vous;
        
    void* f(void* arg){
        printf("Début phase 1 de %d\n",*((int*)arg));
        for(int i = 0; i < 1000; i++){
            ;
        }
        printf("Fin phase 1 de %d\n",*((int*)arg));
        pthread_mutex_lock(&m_compteur);
        compteur = compteur + 1;
        if (compteur == N){
            sem_post(&rendez_vous);
        }
        pthread_mutex_unlock(&m_compteur);
        sem_wait(&rendez_vous);
        sem_post(&rendez_vous);
        // autre méthode
        // if (compteur == N){
        //     for (int i = 0; i<N; i++){
        //         sem_post(&rendez_vous);
        //     }
        // }
        // pthread_mutex_unlock(&m_compteur);
        // sem_wait(&rendez_vous);
        printf("Début phase 2 de %d\n",*((int*)arg));
        for(int i = 0; i<1000; i++){
            ;
        }
        printf("Fin phase 2 de %d\n",*((int*)arg));
        return NULL;
    }
     
    int main(){
        sem_init(&rendez_vous,0,0);
        pthread_t tab[N];
        int num[N];
        for(int i = 0; i<N; i++){
            num[i] = i;
            pthread_create(&(tab[i]),NULL,&f,&(num[i]));
        }
        for( int i = 0; i<N; i++){
            pthread_join(tab[i], NULL);
        }
        return 0;
    }
    \end{lstC}
\end{implementation}


Commentaire des implémentations (une en commentaire, une sans commentaire).\\
On a un compteur du nombre de fils ayant fini la phase 1. C'est une variable partagée, on la protège par un mutex.\\\\
Pour attendre le rendez-vous, on utilise un sémaphore de comtpeur initial nul (les fils ne doivent pas attendre tant que \notion{post} n'a pas été fait). \\
Le $N$-ème fil à incrémenter le compteur doit signaler aux autres qu'ils peuvent passer à la deuxième phase~:
\begin{itemize}
    \item Soit il réveille un fil en attente (un post qui débloque un wait) et chaque fil réveille ensuite un autre avec post. Par propagation, ils se réveillent tous.
    \item Soit le fil fait $N$ fois post pour donner $N$ autorisations de passer.
\end{itemize}

\subsection{Producteur-consommateur}
On considère une situation où deux types de fils d'exécutions travaillent en boucle~:
\begin{itemize}
    \item les \notion{produteurs} : Ils produisent une ressource puis la dépose dans une case disponible s'il y en a une
    \item les \notion{consommateurs} : Ils prennent une des ressources disponibles et la "consomment". Une ressource n'est consommée qu'une fois, on a une nouvelle case libre après.
\end{itemize}

\begin{implementation}{fonctionnement du producteur}
    \begin{lstLNat}
    producteur():
        tant que vrai:
            r = une ressource produit
            ranger r dans une case libre
    \end{lstLNat}
\end{implementation}

\begin{implementation}{fonctionnement du consommateur}
    \begin{lstLNat}
    consommateur():
        tant que vrai:
            prendre une ressource dans une case pleine et la consommer
            libérer la case
        
    \end{lstLNat}
\end{implementation}

code à insérer.

\subsection{Problème du dîner de philosophes}

Des philosophes sont réunis autour d'une table ronde, chacun devant une assiette de spaghettis. Il y a une fourchettes entre deux assiettes consécutives, qui peut être utilisée par le philosophe à droite et le philosophe à gauche uniquement.\\ \textit{voir Fig.5}
Un philosophe a besoin de deux fourchettes, la droite et la gauche, pour manger.\\
Les fourchettes sont des ressources partagées, pas par tous les philosophes mais uniquement par deux philosophes adjacents.\\
Un philosophe alterne entre deux "états" : manger et penser. Lorsqu'il veut manger, il essaye de prendre ses deux fourchettes.\\ \\

\begin{implementation}{première approche}
    On numérote $(\Phi_i)_{i \in \intint{0}{N-1}}$ les philosophes, $(\psi_j)_{j \in \intint{0}{N-1}}$ les fourchettes.\\
    On protège chaque fourchettes par un mutex : \code{fourchettes[i]} est un mutex pour la fourchette \code{i}.
    \begin{lstLNat}
    philosophe(i): //i est le numéro du philosophe
        while(true):
            penser()
            lock(fourchettes[i]) // ça cause un problème
            lock(fourchettes[i+1 mod $N$])
            manger()
            unlock(fourchettes[i+1 mod $N$])
            unlock(fourchettes[i])
    \end{lstLNat}
\end{implementation}
Dans le cas où tous les philosophes se retrouvent au niveau du problème dans le code, tous les mutex (\code{fourchettes[i]}) sont verrouillés et tous les fils sont obligés d'attendre.\\
Tous les philosophes ont pris leur fourchette droite et attendent que la fourchette gauche soit rendue disponible.\\
On peut représenter la situation à l'aide d'un \notion{graphe de requêtes} (HP) $G = (S,A)$ où~:
\begin{itemize}
    \item $S$ est l'ensemble des ressources (protégées par des mutex ou des sémaphores)
    \item $A$ est un ensemble d'arêtes $(a,b)$ telles que s'il existe un fil d'exécution qui, ayant acquis la ressource $a$ sans l'avoir libérée, demande l'accès à la ressource $b$.
    Le graphe ne correspond pas à un quelconque entrelacement, il ne tient compte que de l'ordre des requêtes.
\end{itemize}
Avec ce protocole, le graphe de requête est le suivant. \textit{Voir Fig.6}. Le graphe de requête est cyclique.

\begin{remarque}{}{graphe de requête}
    On peut montrer que si le graphe de requêtes est acyclique, il n'y a pas d'interblocage possible.\\
    Le cas échéant, en effectuant un tri topologique des sommets, parmi tous les fils demandeurs d'accès simultanément, il y a au moins un fil qui demande l'accès à une ressource disponible (celle qui correspond au plus grand sommet dans l'ordre topologique : le plus "vieux parent"). \textit{ce n'est qu'une idée de démonstration}
\end{remarque}

\begin{remarque}{}{méthode d'utilisation du graphe de requête}
    \\begin{enumeratebf}
        \item construction du graphe
        \item \begin{itemize}
            \item s'il est acyclique, absence d'interblocage garantie
            \item sinon, on cherche autrement (diagrammes de transitions ou exhibition d'une situation d'interblocage)
        \end{itemize}
    \end{enumeratebf}
\end{remarque}

\begin{implementation}{deuxième approche}
    Il suffit de modifier le comportement d'un philosophe pour rendre le graphe de requête acyclique (on bouge une flèche : \textit{Fig.6 : correction})
    \begin{lstLNat}
    philosophe(i):
        while(true):
            penser()
            if(i = N-1){
                lock(fourchettes[0])
                lock(fourchettes[N-1])
                manger()
                unlock(fourchettes[N-1])
                unlock(foruchettes[0])
            }
            else{
                lock(fourchettes[i])
                lock(foruchettes[i+1])
                manger()
                unlock(fourchettes[i+1])
                unlock(fourchettes[i])
            }
    \end{lstLNat}
\end{implementation}
Le graphe de requête est maintenant acyclique (\textit{Voir fig.5}) : pas d'interblocage.

\input{../../stock/pied.tex}