\input{../../stock/en-tete_v4.tex}
\begin{document}
\begin{adjustwidth}{-3cm}{-3cm}
\input{../../stock/commands.tex}

\begin{definition}{5.1}{apprentissage}
    L'\notion{apprentissage} en informatique permet l'approche de différents problèmes~:
    \begin{itemize}
        \item la \notion{classification}, par exemple déterminer un objet sur une image, ou un son sur un flux audio ;
        \item la \notion{régression}, par exemple prévoir la valeur du cours de la bourse.
    \end{itemize}
\end{definition}

\begin{definition}{5.2}{apprentissage supervisé}
    L'\notion{apprentissage supervisé} consiste à entraîner un modèle ou un algorithme à l'aide d'un \notion{ensemble d'apprentissage}~:
    $$S = \{(x_i,\,  y_i),\, i \in \intint{1}{n}\} \subset X \times Y \quad \text{avec}\,\begin{cases*}
        X &l'ensemble des \notion{objets} manipulés par le modèle ou l'algorithme\\
        Y &l'ensemble des \notion{classes} ou \notion{valeurs} associées aux objets de $X$
    \end{cases*}$$
    $(x,\, y) \in S$ signifie que \notion{l'objet $x$ est dans la classe $y$} ou \notion{a pour valeur $y$}.\\ 
    On cherche pour un objet inconnu $x$ à déterminer une classe ou une valeur $y$ convenable en s'appuyant sur l'ensemble d'apprentissage $S$.
\end{definition}

\begin{definition}{5.3}{fonction de prédiction}
    À tout modèle ou algorithme d'apprentissage supervisé on peut associer une \notion{fonction $f:X \to Y$ dite de prédiction} qui à un objet associe la \notion{classe estimée raisonnable par le modèle ou l'algorithme}.
\end{definition}

\begin{definition}{5.4}{fonction de perte}
    À tout modèle ou algorithme d'apprentissage supervisé on peut associer une fonction $L:Y^2 \to \R_+$ qui à une prédiction associe une \notion{valeur mesurant son inexactitude}. On a~:
    $$\forall y \in Y,\, L(y,\, y) = 0$$
\end{definition}

\begin{definition}{5.5}{fonction de risque empirique}
    Pour tout modèle ou algorithme d'apprentissage supervisé on peut associer à toute fonction $f$ de prédiction une espérance appelée \notion{risque $R$} par~:
    \begin{align*}
        R(f) &= \mb{E}_{X,Y}\Big( L\big( Y,\, f(X) \big) \Big) \\
        &= \sum_{(x,y) \in X\times Y} L \big( y, f(x) \big) \mb{P}_{X,Y}(x,y)
    \end{align*}
    En pratique, on n'a jamais accès à $\mb{P}_{X,Y}$. On définit alors le \notion{risque empirique comme étant la moyenne des pertes sur l'ensemble d'apprentissage}. Lui est calculable~:
    $$R_\mr{emp}(f) = \frac{1}{\abs{S}} \sum_{(x,y) \in S}L(y,f(x))$$
    Dès lors, un algorithme d'apprentissage supervisé mettra en œuvre des algorithmes d'optimisation afin de trouver une fonction $f$ qui minimise le risque empirique.
\end{definition}

\begin{implementation}{algorithme de classification des $k$ plus proches voisins - classique}
    \begin{itemize}
        \item \textbf{Entrée}~: \begin{itemize}
            \item un ensemble d'apprentissage $S \subset X\times Y$ indexé sur $\intint{0}{n}$
            \item une distance $\mr{d} : X^2 \to \R_+$
            \item un objet $x$ de classe inconnue
            \item $k$ le nombre de voisins à considérer
            
        \end{itemize}
        \item \textbf{Sortie}~: la classe $y$ du voisin majoritairement présent parmi les $k$ plus proches de $x$
    \end{itemize}
    \begin{lstLNat}
    file = file de priorité max vide
    pour tout $i \in \intint{0}{n}$ :
        si $\abs{\code{file}}$ < $k$ :
            ajouter $x_i$ à file avec la priorité $\mr{d}(x,x_i)$
        sinon :
            si $d(x,x_i)$ < file[0] :
                supprimer le maximum de file
                ajouter $x_i$ à file avec la priorité $\mr{d}(x,x_i)$
    renvoyer la classe majoritaire parmi les éléments de file
    \end{lstLNat}
\end{implementation}

\begin{implementation}{construction d'un arbre $d$-dimensionnel}
    Cet algorithme constitue un prétraîtement de l'ensemble d'apprentissage, pour "faciliter" la recherche des $k$ plus proches voisins en aval. Attention, si $d$ est trop grand, on fait face au fléau de la dimension qui rend la méthode trop peu efficace.
    \begin{itemize}
        \item \textbf{Entrée initiale}~: \begin{itemize}
            \item l'ensemble $X_S \subset X$ pour lequel on connaît la classe de chaque objet
            \item $d$ la dimension de $X_S$
        \end{itemize}
        \item \textbf{Sortie}~: un arbre $d$-dimensionnel décrivant les positions relatives de chaque objet de $X_S$
    \end{itemize}
    \begin{lstLNat}
    Construit($i$, X_courant) :
        n = $\abs{\code{X\_courant}}$
        si n == 0 :
            renvoyer Vide
        sinon :
            X_courant = tri de X_courant dans l'ordre croissant de 
                    la $i$-ème coordonnée
            val = X_courant[$\lfloor \frac{n}{2} \rfloor$]
            X_g, X_d = séparer strictement X_courant en deux listes, 
                    en l'indice $\lfloor \frac{n}{2} \rfloor$
            g = Construit($d$, $i+1 \mod d$, X_g)
            d = Construit($d$, $i+1 \mod d$, X_d)
            renvoyer Noeud(val, g, d)
    \end{lstLNat}
\end{implementation}

\begin{implementation}{algorithme de classification des $k$ plus proches voisins - avec arbre $d$-dimensionnel (1/2)}
    \begin{itemize}
        \item \textbf{Entrée initiale}~: \begin{itemize}
            \item un arbre $d$-dimensionnel décrivant les positions relatives de chaque objet de $X_S$
            \item un objet $x$ de classe inconnue
            \item $k$ le nombre de voisins à considérer
            
        \end{itemize}
        \item \textbf{Sortie}~: une pile contenant le chemin de la racine vers une feuille de $\mc{A}$ modélisant le plus proche voisin de $x$.
    \end{itemize}
    \begin{lstLNat}
    explore($\mc{A}$, $i$, $x$) :
        si $\mc{A}$ est Vide :
            renvoyer []
        sinon :
            on identifie $\mc{A}$ = Noeud(val, g, d)
            $x_i$ = $i$-ème coordonnée de $x$
            $v_i$ = $i$-ème coordonnée de val
            si $x_i$ <= $v_i$ :
                renvoyer $\mc{A}$ :: explore(g, $i+1 \mod d$, $x$)
            sinon : 
                renvoyer $\mc{A}$ :: explore(d, $i+1 \mod d$, $x$)
    \end{lstLNat}
\end{implementation}

\begin{implementation}{algorithme de classification des $k$ plus proches voisins - avec arbre $d$-dimensionnel (2/2)}
    \begin{itemize}
        \item \textbf{Entrée initiale}~: \begin{itemize}
            \item une distance $\mr{d} : X^2 \to \R_+$
            \item \code{l} la pile du chemin parcouru vers le plus proche de $x$, contenant des sommets de $\mc{A}$ un arbre $d$-dimensionnel
            \item un objet $x$ de classe inconnue
            \item $k$ le nombre de voisins à considérer
            
        \end{itemize}
        \item \textbf{Sortie}~: la classe $y$ du voisin majoritairement présent parmi les $k$ plus proches de $x$
    \end{itemize}
    \begin{lstLNat}
    file = file de priorité max vide
    kPPV(l, $i$, $x$, $k$) :
        Noeud(val, g, d), q = depiler l
        $x_{i}$ = $i$-ème coordonnée de $x$
        $v_{i}$ = $i$-ème coordonnée de val
        si file[0].prio < $\mr{d}(x_{i}E_{i},v_{i}E_{i})$ // distance unidimensionnelle : 
                // dépend de $\mr{d}$ mais souvent égal à $\,\abs{x_{i} - v_{i}}$
            si besoin, ajouter/remplacer $x$ à file avec la priorité $\mr{d}(x,\code{val})$
        sinon :
            chemin = explore(Noeud(val, g, d), $i \mod d$, $x$)
            kPPV(chemin, $\abs{\code{chemin}} + i \mod d$, $x$, $k$)
        renvoyer la classe majoritaire parmi les éléments de file
    \end{lstLNat}
\end{implementation}

\input{../../stock/pied.tex}