\input{../../stock/en-tete_v4.tex}
\begin{document}
\begin{adjustwidth}{-3cm}{-3cm}
\input{../../stock/commands.tex}

\begin{definition}{5.1}{apprentissage}
    L'\notion{apprentissage} en informatique permet l'approche de différents problèmes~:
    \begin{itemize}
        \item la \notion{classification}, par exemple déterminer un objet sur une image, ou un son sur un flux audio ;
        \item la \notion{régression}, par exemple prévoir la valeur du cours de la bourse.
    \end{itemize}
\end{definition}

\begin{definition}{5.2}{apprentissage supervisé}
    L'\notion{apprentissage supervisé} consiste à entraîner un modèle ou un algorithme à l'aide d'un \notion{ensemble d'apprentissage}~:
    $$S = \{(x_i,\,  y_i),\, i \in \intint{1}{n}\} \subset X \times Y \quad \text{avec}\,\begin{cases*}
        X &l'ensemble des \notion{objets} manipulés par le modèle ou l'algorithme\\
        Y &l'ensemble des \notion{classes} ou \notion{valeurs} associées aux objets de $X$
    \end{cases*}$$
    $(x,\, y) \in S$ signifie que \notion{l'objet $x$ est dans la classe $y$} ou \notion{a pour valeur $y$}.\\ 
    On cherche pour un objet inconnu $x$ à déterminer une classe ou une valeur $y$ convenable en s'appuyant sur l'ensemble d'apprentissage $S$.
\end{definition}

\begin{definition}{5.3}{fonction de prédiction}
    À tout modèle ou algorithme d'apprentissage supervisé on peut associer une \notion{fonction $f:X \to Y$ dite de prédiction} qui à un objet associe la \notion{classe estimée raisonnable par le modèle ou l'algorithme}.
\end{definition}

\begin{definition}{5.4}{fonction de perte}
    À tout modèle ou algorithme d'apprentissage supervisé on peut associer une fonction $L:Y^2 \to \R_+$ qui à une prédiction associe une \notion{valeur mesurant son inexactitude}. On a~:
    $$\forall y \in Y,\, L(y,\, y) = 0$$
\end{definition}

\begin{definition}{5.5}{fonction de risque empirique}
    Pour tout modèle ou algorithme d'apprentissage supervisé on peut associer à toute fonction $f$ de prédiction une espérance appelée \notion{risque $R$} par~:
    \begin{align*}
        R(f) &= \mb{E}_{X,Y}\Big( L\big( Y,\, f(X) \big) \Big) \\
        &= \sum_{(x,y) \in X\times Y} L \big( y, f(x) \big) \mb{P}_{X,Y}(x,y)
    \end{align*}
    En pratique, on n'a jamais accès à $\mb{P}_{X,Y}$. On définit alors le \notion{risque empirique comme étant la moyenne des pertes sur l'ensemble d'apprentissage}. Lui est calculable~:
    $$R_\mr{emp}(f) = \frac{1}{\abs{S}} \sum_{(x,y) \in S}L(y,f(x))$$
    Dès lors, un algorithme d'apprentissage supervisé mettra en œuvre des algorithmes d'optimisation afin de trouver une fonction $f$ qui minimise le risque empirique.
\end{definition}

\begin{implementation}{algorithme de classification des $k$ plus proches voisins - classique}
    \begin{itemize}
        \item \textbf{Entrée}~: \begin{itemize}
            \item un ensemble d'apprentissage $S \subset X\times Y$ indexé sur $\intint{0}{n}$
            \item une distance $\mr{d} : X^2 \to \R_+$
            \item un objet $x$ de classe inconnue
            \item $k$ le nombre de voisins à considérer
            
        \end{itemize}
        \item \textbf{Sortie}~: la classe $y$ du voisin majoritairement présent parmi les $k$ plus proches de $x$
    \end{itemize}
    \begin{lstLNat}
    file = file de priorité max vide
    pour tout $i \in \intint{0}{n}$ :
        si $\abs{\code{file}}$ < $k$ :
            ajouter $x_i$ à file avec la priorité $\mr{d}(x,x_i)$
        sinon :
            si $d(x,x_i)$ < file[0] :
                supprimer le maximum de file
                ajouter $x_i$ à file avec la priorité $\mr{d}(x,x_i)$
    renvoyer la classe majoritaire parmi les éléments de file
    \end{lstLNat}
\end{implementation}

\begin{implementation}{construction d'un arbre $d$-dimensionnel}
    Cet algorithme constitue un prétraîtement de l'ensemble d'apprentissage, pour "faciliter" la recherche des $k$ plus proches voisins en aval. Attention, si $d$ est trop grand, on fait face au fléau de la dimension qui rend la méthode trop peu efficace.
    \begin{itemize}
        \item \textbf{Entrée}~: \begin{itemize}
            \item l'ensemble $X_S \subset X \subset \R^D$ pour lequel on connaît la classe de chaque objet
            \item $i \in \intint{0}{D-1}$ la coordonnée selon laquelle on trie
        \end{itemize}
        \item \textbf{Sortie finale}~: un arbre $D$-dimensionnel décrivant les positions relatives de chaque objet de $X_S$
    \end{itemize}
    \begin{lstLNat}
    Construit($i$, $X_S$) :
        n = $\abs{X_S}$
        si n == 0 :
            renvoyer Vide
        sinon :
            $X_S$ = tri de $X_S$ dans l'ordre croissant de 
                    la $i$-ème coordonnée
            val = $X_S$[$\lfloor \frac{n}{2} \rfloor$]
            $X_g$, $X_d$ = séparer strictement $X_S$ en l'indice $\lfloor \frac{n}{2} \rfloor$
            g = Construit($d$, $i+1 \mod D$, $X_g$)
            d = Construit($d$, $i+1 \mod D$, $X_d$)
            renvoyer Noeud(val, g, d)
    \end{lstLNat}
\end{implementation}

\begin{implementation}{algorithme de classification des $k$ plus proches voisins - avec arbre $D$-dimensionnel (1/2)}
    \begin{itemize}
        \item \textbf{Entrée}~: \begin{itemize}
            \item un arbre $D$-dimensionnel décrivant les positions relatives de chaque objet de $S \subset X \times Y$, avec $X \subset \R^D$
            \item $i \in \intint{0}{D-1}$ la coordonnée courante
            \item un objet $x$ de classe inconnue
        \end{itemize}
        \item \textbf{Sortie}~: une pile contenant le chemin de la racine (en queue) vers une feuille de $\mc{A}$ modélisant le plus proche voisin de $x$ (en tête).
    \end{itemize}
    \begin{lstLNat}
    explore($\mc{A}$, $i$, $x$) :
        si $\mc{A}$ est Vide :
            renvoyer []
        sinon :
            on identifie $\mc{A}$ = Noeud(val, g, d)
            $x_i$ = $i$-ème coordonnée de $x$
            $v_i$ = $i$-ème coordonnée de val
            si $x_i$ <= $v_i$ :
                renvoyer $\mc{A}$ :: explore(g, $i+1 \mod D$, $x$)
            sinon : 
                renvoyer $\mc{A}$ :: explore(d, $i+1 \mod D$, $x$)
    \end{lstLNat}
\end{implementation}

\begin{implementation}{algorithme de classification des $k$ plus proches voisins - avec arbre $D$-dimensionnel (2/2)}
    On munit $X \subset R^D$ d'une distance $\mr{d} : X^2 \to \R_+$
    \begin{itemize}
        \item \textbf{Entrée initiale}~: \begin{itemize}
            \item une pile contenant le chemin de la racine (en queue) vers une feuille de $\mc{A}$ modélisant le plus proche voisin de $x$ (en tête)
            \item $i \in \intint{0}{D-1}$ la coordonnée courante
            \item un objet $x$ de classe inconnue
            \item $k$ le nombre de voisins à considérer
            
        \end{itemize}
        \item \textbf{Sortie}~: la classe $y$ du voisin majoritairement présent parmi les $k$ plus proches de $x$
    \end{itemize}
    \begin{lstLNat}
    file = file de priorité max vide
    kPPV(l, $i$, $x$, $k$) :
        Noeud(val, g, d), new_l = depiler l
        $x_{i}$ = $i$-ème coordonnée de $x$
        $v_{i}$ = $i$-ème coordonnée de val
        si file[0].prio < $\mr{d}(x_{i}E_{i},v_{i}E_{i})$ // distance unidimensionnelle : 
                // dépend de $\mr{d}$ mais souvent égal à $\,\abs{x_{i} - v_{i}}$
            si $\abs{\code{file}}$ < k :
                ajouter $x$ à la file avec la priorité $\mr{d}(x,\code{val})$
            sinon :
                si file[0].prio > $\mr{d}(x,\code{val})$ :
                    supprimer l'élément de priorité maximale de file
                    ajouter $x$ à la file avec la priorité $\mr{d}(x,\code{val})$

        sinon : // risque de voisins plus proches, 
                // par lesquels on ne passerait pas : on va regarder
            chemin = explore(Noeud(val, g, d), $i \mod D$, $x$)
            kPPV(chemin, $\abs{\code{chemin}} + i \mod D$, $x$, $k$)
         
        si $\abs{\code{file}}$ < k : // besoin d'encore des voisins ?
            kPPV(new_l, $i-1 \mod D$, $x$, $k$) // on remonte l'arbre

        renvoyer la classe majoritaire parmi les éléments de file
    \end{lstLNat}
\end{implementation}

\begin{definition}{5.6}{entropie de Shannon}
    Soit $S = \{(x_i,y_i),\, i \in \intint{1}{n}\} \subset X \times Y$ un ensemble d'apprentissage. Pour $y \in Y$, on définit \notion{$C_y = \{x \in X,  (x,y) \in S\}$ l'ensemble des objets de classe $y$}.\\
    L'\notion{entropie de Shannon} de $S$ est~:
    $$H(S) = - \sum_{\substack{y \in Y\\C_y \neq \varnothing}}\frac{\abs{C_y}}{n} \ln\Big(\frac{\abs{C_y}}{n}\Big)$$
    L'entropie de Shannon mesure le désordre dans la distribution des classes de $S$.
\end{definition}

\begin{definition}{5.7}{gain d'information}
    Soit $S \subset X \times Y$ un ensemble d'apprentissage, avec $X \subset A_1 \times \dots \times A_m \times X'$. Pour $i \in \intint{1}{m}$, On considère le critère $A_i$ pouvant prendre les valeurs $v_1,\, \dots,\, v_k$. On écrit alors~:
        \begin{align*}S &= \bigsqcup_{j=1}^k \big\{(x,y) \in S,\, \underbrace{x_i}_{\substack{\text{valeur du critère} \\A_i \text{ pour } x}} = v_j\big\} \\
            &= \bigsqcup_{j=1}^k S_j \quad \text{(notation)}
        \end{align*}
    On définit \notion{le gain d'information du critère $A_i$} comme~:
    $$G(S,A_i) = H(S) - \sum_{j=1}^{k} \frac{\abs{S_j}}{\abs{S}} H(S_j)$$
    Le gain d'information \notion{renseigne sur la pertinence du critère $A_i$ pour discriminer les objets de $S$}.
\end{definition}

\begin{definition}{5.8}{apprentissage non supervisé}
    L'apprentissage non supervisé repose des méthodes de \notion{clustering} ou (ou \notion{méthode de classifcation non supervisée}). Étant donné un ensemble $z = \{x_i,\, i \in \intint{1}{n}\}$, on en cherche une partition~:
    $$z = \bigsqcup_{i=1}^kP_i$$
    Les $(P_i)_{i \in \intint{1}{k}}$ sont appelés \notion{classes} ou \notion{clusters}. L'objectif est que ces clusters vérifient~:
    \begin{itemize}
        \item petite variabilité intra-classe
        \item grande variabilité inter-classe
    \end{itemize}
\end{definition}

\begin{definition}{5.9}{relation de finesse}
    Soit $z$ un ensemble d'objets, de partitions en clusters $(P_i)_{i\in\intint{1}{k}}$ et $(P_j')_{j\in\intint{1}{k'}}$. On dit que  \notion{$(P_i)_{i\in\intint{1}{k}}$ est plus fine que $(P_j')_{j\in\intint{1}{k'}}$} lorsque \notion{tout cluster $P_j'$ est l'union de clusters $P_i$}.
\end{definition}

\begin{definition}{5.10}{variance d'une partition}
    Soit $z$ un ensemble d'objets d'un espace métrique $(E, \mr{d})$, de partition en clusters $(P_i)_{i\in\intint{1}{k}}$. \notion{La variance de la partition $(P_i)_{i\in\intint{1}{k}}$} est~:
    $$V\Big((P_i)_{i\in\intint{1}{k}}\Big) = \sum_{i=1}^{k}\sum_{x \in P_i}\mr{d}(x,g_i)$$
    où, pour tout $i \in \intint{1}{k}$, \notion{$g_i$ est le barycentre du cluster $P_i$}~:
    $$g_i = \frac{1}{\abs{P_i}}\sum_{x \in P_i}x$$
\end{definition}

\begin{definition}{5.11}{hiérarchie d'un ensemble d'objets}
    Soit $z$ un ensemble d'objets. On \notion{appelle hiérarchie de $z$} toute famille finie $(\pi_i)_{i \in \intint{1}{n}}$ de partitions de $z$ telle que~:
    \begin{enumeratebf}
        \item $\forall i\in \intint{1}{n-1},\, \pi_i \text{ est plus fine que } \pi_{i+1}$
        \item $\pi_1$ est celle des singletons de $z$
        \item $\pi_n = \{z\}$
    \end{enumeratebf}
    On représente une hiérarchie à l'aide d'un \notion{dendrogramme}.
\end{definition}

\begin{implementation}{algorithme de classification hiérarchique ascencendante}
    L'algorithme de CHA permet de renvoyer une partition en un nombre raisonnable de clusters d'un ensemble $Z$ d'objets. On utilise Unir & Trouver pour modéliser la relation "appartenir au même ensemble que".
    \begin{itemize}
        \item \textbf{Entrée}~: \begin{itemize}
            \item $Z$ le tableau des objets considérés
            \item $\mb{D}:\mc{P}(Z)^2 \to \R_+$ un écart sur $Z$
        \end{itemize}
        \item \textbf{Sortie}~: une partition de $H$ en un nombre raisonnable de clusters
    \end{itemize}
    \begin{lstLNat}
    CHA($Z$,$\mb{D}$):
        H = tableau de taille $\abs{Z}$ rempli de $\varnothing$
        P = tableau de taille $\abs{Z}$ contenant les structures Unir & Trouver singletons de chaque objet.
        H[0] = copier P profondément
        ecarts = tableau de taille $\abs{Z} - 1$ rempli de $-1$
        
        // construction de la hiérarchie
        pour p allant de 1 à $\abs{Z}-1$ :
            (i,j) = (0,1)
            pour k allant de 0 à $\abs{Z}-1$ :
                pour l allant de k+1 à $\abs{Z}-1$ :
                    si $\mb{D}$(P[i].val, P[j].val) < $\mb{D}$(P[k].val, P[l].val) :
                        (i,j) = (k,l) 
            unir P[i] avec P[j]
            H[p] = copier P profondément
            ecarts[p-1] = $\mb{D}$(P[i].val, P[j].val)

        // parcours de la hiérarchie
        i_ecart_max = 0
        pour p allant de 1 à $\abs{Z}-2$ :
            si ecarts[i_ecart_max] <= ecarts[p] :
                i_ecart_max = p
        
        renvoyer H[i_ecart_max]
    \end{lstLNat}
\end{implementation}

\input{../../stock/pied.tex}